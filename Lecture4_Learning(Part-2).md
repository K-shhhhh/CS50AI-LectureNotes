# ğŸ¤– CS50 AI with Python â€“ Lecture 4: Learning (Part 2)

---

## ğŸ¯ Overfitting

Overfitting occurs when a model fits the training data too well but fails to generalize to new, unseen data.

- Loss functions can be misleading â€” a perfect fit (loss = 0) might still generalize poorly.

### ğŸ§  Example:
In the overfitted model, a dot near a red label might still be incorrectly predicted as red (No Rain), even if it should be blue (Rain), because the model is too specific to training data.

---

## ğŸ›¡ï¸ Regularization

Regularization penalizes model complexity to avoid overfitting.

### ğŸ§® Cost Function with Regularization:

---

## ğŸ§  Reinforcement Learning

Reinforcement learning (RL) teaches an agent to make decisions by providing feedback:

- Reward (**+ve**)
- Punishment (**â€“ve**)

The environment gives:

- State

Agent takes action
Receives new state + reward

---

## ğŸ” Markov Decision Processes (MDPs)

RL can be modeled as MDPs with:

- Set of States **S**
- Actions **A**
- Transition Model **P(s' | s, a)**
- Reward Function **R(s, a, s')**

### ğŸ§  Example:
- Grid world
- Agent = Yellow circle
- Goal = Reach green square while avoiding red
- Each square = state
- Moving in any direction = action
- Red = negative reward, Green = positive reward

---

## ğŸ§® Q-Learning

Q-learning learns the value **Q(s, a)** â€” expected utility of taking action **a** in state **s**.

### ğŸ”§ Update Rule:

- Q(s, a) â† Q(s, a) + Î± * (r + Î³ * max(Q(s', a')) - Q(s, a))

- Where:

- **Î±**: learning rate
- **Î³**: discount factor
- **r**: reward
- **s'**: new state

### ğŸ’¡ Explore vs Exploit:
- **Exploit**: Choose best known action
- **Explore**: Try new actions for better learning

### ğŸ§  Îµ-greedy algorithm:
- With probability **Îµ**: random action (explore)
- With probability **1-Îµ**: best known action (exploit)

### ğŸ”¢ Q-Learning Example 1: Gridworld
- States: Grid cells
- Actions: Up, Down, Left, Right
- Rewards:
  - +10 for reaching goal
  - -1 per move
  - -10 for hitting wall or trap
- Agent learns which path gives maximum reward

### ğŸ”¢ Q-Learning Example 2: Game of Nim
- Train agent via self-play
- Reward: +1 for win, -1 for loss
- Initially plays randomly
- After thousands of games, learns optimal strategy

---

## ğŸ“ˆ Function Approximation

In complex games like Chess, explicitly storing **Q(s, a)** is infeasible.

### Solution:
- Use function approximators (e.g., Neural Networks)
- Generalize Q-values from similar states/actions

---

## ğŸ§  Unsupervised Learning

Unsupervised learning finds patterns in unlabeled data.

### ğŸ”— Clustering
Groups similar data points together.

### ğŸ§  Use cases:
- Genetics (similar genes)
- Image segmentation (similar pixel regions)

---

## ğŸ“Š k-means Clustering

### Algorithm:
- Randomly initialize **k** cluster centers
- Assign each point to nearest cluster center
- Recalculate cluster centers as mean of assigned points
- Repeat until assignments no longer change

---

## âœ… Summary Table of Learning (Part 2)

| Topic                  | Type         | Notes                                   |
| :--------------------- | :----------- | :-------------------------------------- |
| Overfitting            | Supervised   | Model fits training data too closely    |
| Regularization         | Supervised   | Penalize complex models                 |
| Cross Validation       | Supervised   | Evaluate generalization                 |
| scikit-learn           | Supervised   | Popular ML library                      |
| Reinforcement Learning | Reinforcement| Feedback-based learning via reward/punishment |
| Q-Learning             | Reinforcement| Value-based learning, epsilon-greedy policy |
| Function Approximation | Reinforcement| Generalize across similar states/actions|
| Clustering             | Unsupervised | Group data without labels               |
| k-means                | Unsupervised | Partition data into k clusters 