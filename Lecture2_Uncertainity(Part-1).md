# ğŸ¤– CS50 AI with Python â€“ Lecture 2: **Uncertainty & Probability**

This lecture explores how AI handles uncertainty using **probability theory**, enabling intelligent decisions even with incomplete information.

---

## ğŸ§  Why Uncertainty?

Real-world decisions often lack complete knowledge.  
For example:  
> "Whatâ€™s the weather tomorrow?"  
We donâ€™t *know* for sure â€” but we can **predict** using **probabilities**.

---

## ğŸ² Basic Concepts of Probability

### ğŸŒ Possible Worlds (`Ï‰`)
Each outcome is a *possible world*.  
E.g., rolling a die gives:
```
Ï‰ âˆˆ {1, 2, 3, 4, 5, 6}
```
> Probability of world `Ï‰`: `P(Ï‰)`

### ğŸ“ Axioms of Probability

| Rule | Explanation |
|------|-------------|
| `0 â‰¤ P(Ï‰) â‰¤ 1` | All probabilities between 0 and 1 |
| `P(impossible)` = 0 | e.g., rolling a 7 on a 6-sided die |
| `P(certain)` = 1 | e.g., getting < 10 on a 6-sided die |
| Sum of all `P(Ï‰)` = 1 | Total probability is 1 |

---

## ğŸ² Dice Example: Summing Probabilities

Rolling **two dice** = 36 total outcomes  
- P(sum = 12) â†’ only `(6,6)` â†’ `1/36`  
- P(sum = 7)  â†’ six outcomes â†’ `6/36 = 1/6`

---

## ğŸ” Types of Probability

### âœ… Unconditional Probability
- Based on no prior evidence  
> e.g., P(rain) = 0.1

### ğŸ”„ Conditional Probability

> P(a | b) = â€œProbability of a **given** b is trueâ€

### ğŸ“˜ Formula:

```
P(a | b) = P(a âˆ§ b) / P(b)
```

#### ğŸ¯ Example:
> What is P(sum = 12 | first die is 6)?

- Outcomes where die1 = 6: `{(6,1), (6,2), â€¦ (6,6)}`
- Only one of them (6,6) has sum = 12
- So, `P(12 | die1 = 6) = 1/6`

---

## ğŸ² Random Variables & Distributions

### ğŸ² What is a Random Variable?
A variable with multiple possible outcomes  
> e.g., `Roll âˆˆ {1, 2, 3, 4, 5, 6}`

### ğŸ“Š Probability Distribution
| Outcome | Probability |
|---------|-------------|
| On Time | 0.6         |
| Delayed | 0.3         |
| Canceled| 0.1         |

â¡ Can be written as vector:  
```
P(Flight) = <0.6, 0.3, 0.1>
```

---

## ğŸ”— Independence

Two events are **independent** if one doesnâ€™t affect the other:
```
P(A âˆ§ B) = P(A) * P(B)
```
E.g. Dice rolls are independent; cloudiness and rain are not.

---

## ğŸ“ Bayesâ€™ Rule

### ğŸ“˜ Formula:
```
P(B | A) = [P(A | B) * P(B)] / P(A)
```

### ğŸ¯ Example: Rain and Clouds
| Event | Value |
|-------|-------|
| P(clouds | rain) | 0.8 |
| P(clouds)        | 0.4 |
| P(rain)          | 0.1 |

> **P(rain | clouds) = (0.8 Ã— 0.1) / 0.4 = 0.2 (20%)**

ğŸ“Œ So even with partial evidence (clouds), we can better predict rain.

---

## ğŸ“˜ Joint Probability

Probability of two or more events happening **together**.

### ğŸŒ¦ Clouds and Rain Table

| Cloud | Rain | P     |
|-------|------|-------|
| âœ…    | âœ…   | 0.08  |
| âœ…    | âŒ   | 0.32  |
| âŒ    | âœ…   | 0.02  |
| âŒ    | âŒ   | 0.58  |

> e.g., `P(cloud âˆ§ rain) = 0.08`

---

## ğŸ” Conditional from Joint

```
P(cloud | rain) = P(cloud âˆ§ rain) / P(rain)
               = 0.08 / 0.10 = 0.8
```

âœ… You can **normalize** values to create distributions:
```
P(cloud | rain) = Î± <0.08, 0.02>
               = <0.8, 0.2>
```

---

## ğŸ“ Probability Rules

### â— Negation:
```
P(Â¬a) = 1 - P(a)
```

### ğŸ”„ Inclusion-Exclusion:
```
P(a âˆ¨ b) = P(a) + P(b) - P(a âˆ§ b)
```

ğŸ“Œ Avoids double-counting!

#### ğŸ¨ Example:
```
P(ice cream) = 0.8  
P(cookies) = 0.7  
P(both) = 0.5  

P(either) = 0.8 + 0.7 - 0.5 = 1.0 âœ…
```

---

## ğŸ§® Marginalization

```
P(a) = P(a âˆ§ b) + P(a âˆ§ Â¬b)
```

â¡ Just add both possibilities of b

---

## âš™ Conditioning (Full Form)

```
P(a) = P(a | b)P(b) + P(a | Â¬b)P(Â¬b)
```

ğŸ§  We're saying:  
> "A can happen **with** B or **without** B"

---

ğŸ“Œ This concludes the **first half** of Lecture 2 â€“ covering all fundamentals of probability in AI.

ğŸ§  Coming up next: **Bayesian Networks & Inference Models**

---
