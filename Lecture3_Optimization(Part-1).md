# ðŸ¤– CS50 AI with Python â€“ Lecture 3: **Optimization (Part 1)**

This lecture focuses on **optimization algorithms**â€”methods used to choose the best option among many possible ones. These are foundational to solving problems efficiently when searching through large solution spaces.

---

## ðŸš€ What is Optimization?
Optimization = Choosing the **best** outcome from possible options.
- Used in **minimax**, **pathfinding**, **resource allocation**, etc.

---

## ðŸ” Local Search
A search algorithm that:
- Starts at a single node (state)
- Moves to **neighboring** states
- Goal: Find a "good enough" answer with **less computation**

### ðŸ¥ Example:
Placing 2 hospitals to minimize Manhattan distances to 4 houses.
- Cost = Sum of distances from each house to the nearest hospital
- State = One configuration of hospital placements

### ðŸ“‰ State-Space Landscape Terms
- **Objective Function**: To **maximize** value
- **Cost Function**: To **minimize** cost
- **Current State**: The active configuration
- **Neighbor State**: Slight variation of current state (e.g., move hospital one cell)

---

## ðŸ§— Hill Climbing
A local search strategy:
- Compare current state to neighbors
- Move to better neighbor (higher value or lower cost)
- Repeat until no neighbor is better

### ðŸ§  Pseudocode:
```python
function Hill-Climb(problem):
    current = initial state
    repeat:
        neighbor = best-valued neighbor of current
        if neighbor not better than current:
            return current
        current = neighbor
```

---

### âš ï¸ Limitation:
- Gets stuck in **local minima** (not global best)
- Example: hospital placement improves from cost 17 â†’ 11 but misses optimal cost of 9

---

## ðŸ“‰ Local vs Global Minima/Maxima

| Term            | Description                     |
|-----------------|---------------------------------|
| Local Maximum   | Better than nearby states       |
| Global Maximum  | Best of all                     |
| Local Minimum   | Worse than nearby states        |
| Global Minimum  | Worst of all                    |

#### Special Cases:
- **Flat Local Max/Min**: Plateau of same-value neighbors  
- **Shoulder**: Plateau with both better and worse neighbors; algorithm stuck in center

---

## ðŸ” Variants of Hill Climbing

| Variant            | Description                                                              |
|--------------------|--------------------------------------------------------------------------|
| Steepest-ascent    | Pick best neighbor (default)                                             |
| Stochastic         | Pick randomly from better neighbors                                      |
| First-choice       | Pick first better neighbor found                                         |
| Random-restart     | Try multiple random starts, pick best result                             |
| Local Beam Search  | Track `k` best states at once                                            |

âž¡ These aim to **reduce chances** of getting stuck, but don't guarantee global optimality.

---

## â„ï¸ Simulated Annealing

Inspired by metal cooling:
- Start with high "temperature" â†’ allow random moves
- Gradually lower temp â†’ fewer random moves
- Can escape **local optima**

### ðŸ”¢ Pseudocode:
```python
function Simulated-Annealing(problem, max):
    current = initial state
    for t = 1 to max:
        T = Temperature(t)
        neighbor = random neighbor
        Î”E = neighbor value - current value
        if Î”E > 0:
            current = neighbor
        else with probability e^(Î”E/T):
            current = neighbor
    return current
```

- `e â‰ˆ 2.72`, Î”E < 0 means worse
- High `T` â†’ more likely to accept worse neighbor
- Low `T` â†’ less likely

---

## ðŸ§­ Traveling Salesman Problem (TSP)

- Find shortest route visiting all cities and returning
- 10 points â†’ 10! = 3.6 million routes
- Simulated annealing gives **good solutions** efficiently

---

## ðŸ“ Linear Programming (LP)

Optimize **linear cost functions** with constraints:

### âœï¸ Components
- **Cost Function**: `c1*x1 + c2*x2 + ...`
- **Constraints**:
  - Inequality: `a1*x1 + a2*x2 + ... â‰¤ b`
  - Equality: `a1*x1 + a2*x2 + ... = b`
- **Bounds**: `li â‰¤ xi â‰¤ ui`

---

### ðŸ’¡ Example

- 2 machines: X1 ($50/hr), X2 ($80/hr)
- Minimize cost â†’ `50x1 + 80x2`

Constraints:
- Labor: `5x1 + 2x2 â‰¤ 20`
- Output: `10x1 + 12x2 â‰¥ 90` â†’ rewritten as `-10x1 -12x2 â‰¤ -90`

---

### ðŸ Python Code:
```python
import scipy.optimize

result = scipy.optimize.linprog(
    [50, 80],
    A_ub=[[5, 2], [-10, -12]],
    b_ub=[20, -90]
)

if result.success:
    print(f"X1: {round(result.x[0], 2)} hours")
    print(f"X2: {round(result.x[1], 2)} hours")
else:
    print("No solution")
```

âž¡ Libraries like `scipy.optimize` implement solvers like **Simplex** and **Interior-Point**.

---

ðŸ“Œ This concludes **Part 1** of Lecture 3 â€“ covering:
- Local Search
- Hill Climbing
- Simulated Annealing
- TSP
- Linear Programming

ðŸ§  Next up: **Continuous Optimization & Gradient Descent**