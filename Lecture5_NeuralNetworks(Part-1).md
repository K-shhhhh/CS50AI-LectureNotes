# ðŸ¤– CS50 AI with Python â€“ Lecture 5: Neural Networks (Part 1)

---

## ðŸ§  Inspiration from Biology

Neural Networks in AI are inspired by the human brain:
- Neurons in the brain receive and send signals.
- When input exceeds a threshold, the neuron activates.

---

## ðŸ’» Artificial Neural Networks (ANN)

ANNs simulate this idea:
- Each **unit** (analogous to a neuron) receives **inputs**.
- Each input is multiplied by a **weight**.
- A **bias** is added.
- The result is passed through an **activation function**.
- The network **learns weights** by training on data.

### ðŸ”§ Hypothesis Function:

- h(xâ‚, xâ‚‚) = wâ‚€ + wâ‚Â·xâ‚ + wâ‚‚Â·xâ‚‚


---

## âš¡ Activation Functions

Used to decide the output based on weighted input sum.

### ðŸ“‰ Step Function:
- Outputs 0 before threshold, 1 after.

### ðŸ“ˆ Logistic Function (Sigmoid):
- Outputs a value between 0 and 1 (confidence score).

### ðŸ§± ReLU (Rectified Linear Unit):
- If value > 0 â†’ output value  
- If value < 0 â†’ output 0

---

## ðŸ”— Simple Neural Network Structure

A simple network:
- **Inputs**: xâ‚, xâ‚‚
- **Weights**: wâ‚, wâ‚‚
- **Bias**: wâ‚€
- **Output**: g(wâ‚€ + wâ‚xâ‚ + wâ‚‚xâ‚‚)

---

## ðŸ”¢ Logical Operations as Neural Networks

### ðŸŸ  OR Function:

| xâ‚ | xâ‚‚ | f(xâ‚, xâ‚‚) |
|----|----|-----------|
| 0  | 0  |     0     |
| 0  | 1  |     1     |
| 1  | 0  |     1     |
| 1  | 1  |     1     |

#### Neural Network Representation:

- Output = g(-1 + xâ‚ + xâ‚‚), threshold = 0

- If sum â‰¥ 0 â†’ Output 1
- Else â†’ Output 0

### ðŸŸ¢ AND Function:
- Similar, but bias = -2

---

## ðŸŒ¦ï¸ Real-world Examples

- Inputs: Humidity, Pressure
- Output: Probability of Rain

OR

- Inputs: Ad Spend, Month
- Output: Predicted Sales

---

## ðŸ§® Gradient Descent

Used to **train weights** to minimize loss.

### ðŸªœ Basic Algorithm:
1. Start with random weights
2. Repeat:
   - Compute **gradient** of loss wrt weights
   - Update weights in direction that **reduces loss**

---

## ðŸ”„ Variants of Gradient Descent

- **Batch GD**: Uses all data â†’ accurate but slow
- **Stochastic GD**: Uses 1 sample â†’ fast but noisy
- **Mini-Batch GD**: Uses small subsets â†’ best of both

---

## ðŸŒ¤ï¸ Example: Weather Prediction

- Inputs: xâ‚ â€¦ xâ‚™
- Outputs: Sunny, Rainy, Cloudy
- Each output = its own small neural network with independent weights

---

## âž— Linear vs Non-linear Models

- Simple perceptron can only learn **linear** boundaries
- Many real-world problems need **non-linear** decision boundaries

---

## ðŸ§  Multilayer Neural Networks (MLP)

Adds **hidden layers** between input and output

### ðŸ”§ Structure:
- Input Layer
- One or more **Hidden Layers**
- Output Layer

### ðŸ”„ Flow:
- Inputs â†’ Hidden Layer(s) â†’ Outputs
- Each layer processes & passes forward weighted values

---

## ðŸ” Backpropagation

Main training algorithm for deep networks.

### ðŸ§  Steps:
1. Calculate **error** at output layer
2. Propagate error **backward**:
   - Layer by layer, compute gradient for weights
3. Update weights
4. Repeat

âž¡ Enables Deep Learning: More than 1 hidden layer

---

## ðŸ§  Overfitting & Dropout

Overfitting = Model fits training data too well â†’ Poor generalization

### ðŸ§ª Dropout Technique:
- Randomly deactivate some units during training
- Prevents reliance on specific neurons
- Encourages network robustness

âž¡ After training, use full network again for prediction

---

ðŸ“Œ Summary

| Concept                  | Description |
|--------------------------|-------------|
| Neural Network           | Input â†’ Weights â†’ Activation â†’ Output |
| Activation Functions     | Step, Sigmoid, ReLU |
| Logical Functions        | OR, AND via weighted inputs |
| Gradient Descent         | Learn weights to minimize loss |
| Multilayer Perceptron    | Enables non-linear decision boundaries |
| Backpropagation          | Core training algorithm |
| Dropout                  | Reduces overfitting |

ðŸ“š Up Next: Deeper architectures, convolutional nets, and applications
