# ğŸ¤– CS50 AI with Python â€“ Lecture 4: Learning (Part 1)

This lecture introduces **Machine Learning (ML)** â€” the ability of computers to learn patterns from data instead of being explicitly programmed.

---

## ğŸ“š What is Machine Learning?

- Machine learning finds **patterns** in historical data and generalizes them for **future predictions**.
- Instead of programming specific behavior, we give examples and let the system **learn the mapping** from inputs to outputs.

---

## ğŸ¯ Supervised Learning

- In **supervised learning**, the algorithm learns from **labeled data** (inputs paired with outputs).
- The goal is to learn a function `f(x)` that maps inputs to the correct outputs.

### Example: Weather Prediction
- Inputs: Humidity, Pressure
- Output: Rain or No Rain
- Learn a function `h(humidity, pressure)` that approximates `f`

---

## ğŸ§ª Classification

- Classification is a type of supervised learning where output is **discrete**.
- The learned model maps inputs to classes like:
  - âœ… Rain
  - âŒ No Rain

---

## ğŸ“ Nearest Neighbor Classification

- **Nearest neighbor**: Classify a new point based on the label of the **closest** point in the training data.
- Can be affected by **outliers**.

### ğŸ§  k-Nearest Neighbors (k-NN)
- Instead of one neighbor, look at **k closest** neighbors.
- Use the **majority label** among those k.
- More robust to noise.
- Drawback: Requires **storing and comparing** to all data points â†’ expensive for large datasets.

---

## ğŸ§® Perceptron

- The perceptron is a **linear classifier**.
- It classifies data by computing a **weighted sum** of the inputs:

### ğŸ”¢ Perceptron Equation

- h(xâ‚, xâ‚‚) =
-   1 if wâ‚€ + wâ‚xâ‚ + wâ‚‚xâ‚‚ â‰¥ 0
-   0 otherwise


- Represent inputs as vector `x = (1, xâ‚, xâ‚‚)`
- Represent weights as vector `w = (wâ‚€, wâ‚, wâ‚‚)`
- Compute `w Â· x` (dot product)

### ğŸ”§ Perceptron Learning Rule

- wáµ¢ = wáµ¢ + Î±(y - h(x)) * xáµ¢


- If prediction is wrong, adjust weights
- Î± is the **learning rate**

---

## ğŸ§± Thresholds

- **Hard Threshold**: Predicts 0 or 1, no in-between
- **Soft Threshold (Sigmoid/Logistic Function)**:
  - Outputs probability (between 0 and 1)
  - Reflects **confidence** in the prediction

---

## ğŸ§­ Support Vector Machines (SVM)

- SVM tries to find the **best boundary** (hyperplane) that **maximizes the margin** between classes.

### Benefits:
- Robust against outliers
- Can handle **non-linear** boundaries using **kernel trick**

---

## ğŸ“ˆ Regression

- **Regression** predicts **continuous values** (e.g., house price, temperature).
- Goal: Fit a **line or curve** to the data that minimizes error.

---

## ğŸ¯ Loss Functions

Used to quantify how **wrong** a prediction is.

### ğŸ§® Classification Loss: 0â€“1 Loss

- L(actual, predicted) =
-   0 if correct
-   1 if incorrect


### ğŸ“‰ Regression Loss:

- **L1 Loss**: `|actual - predicted|`
- **L2 Loss**: `(actual - predicted)Â²`

- **L2 Loss** penalizes outliers more heavily than L1.

---

## ğŸ“Š Summary Table

| Technique       | Type          | Notes                                      |
|----------------|---------------|--------------------------------------------|
| k-NN           | Classification | Uses k nearest neighbors                   |
| Perceptron     | Classification | Linear boundary, simple & fast             |
| SVM            | Classification | Maximizes margin, robust, non-linear OK    |
| Regression     | Regression     | Predicts continuous values, uses loss fn.  |

---

ğŸ“Œ End of Lecture 4 (Part 1): You now understand how computers can learn from data using:
- k-NN
- Perceptrons
- SVM
- Regression
- Loss functions

Next up: **Learning Part 2 â€“ Unsupervised Learning, Neural Networks, and Beyond**
